import express from 'express';
import path from 'node:path';
import { fileURLToPath } from 'node:url';
import cors from 'cors';
import { scrapeListPageAndDetails, scrapeDetail, scrapeAllPagesAndDetails, fetchListLinks } from './scraper.js';
import { appendRows } from './sheets.js';
import { config } from 'dotenv';

config(); // Charger les variables d'environnement

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

const app = express();
const PORT = process.env.PORT || 3000;

app.use(cors());
app.use(express.json());

// Routes API (AVANT les fichiers statiques et le fallback)
app.get('/api/ping', (req, res) => {
  res.json({ ok: true, message: 'pong', timestamp: new Date().toISOString() });
});

app.get('/api/scrape', async (req, res) => {
  const { url, limit } = req.query;
  try {
    const data = await scrapeListPageAndDetails(url, limit ? Number(limit) : 50);
    res.json({ ok: true, count: data.length, data });
  } catch (e) {
    res.status(500).json({ ok: false, error: e?.message || String(e) });
  }
});

app.get('/api/scrape/detail', async (req, res) => {
  const { url } = req.query;
  if (!url) return res.status(400).json({ ok: false, error: 'url is required' });
  try {
    const data = await scrapeDetail(url);
    res.json({ ok: true, data });
  } catch (e) {
    res.status(500).json({ ok: false, error: e?.message || String(e) });
  }
});

app.post('/api/export', async (req, res) => {
  const { spreadsheetId, range, rows } = req.body || {};
  if (!spreadsheetId || !range || !Array.isArray(rows)) {
    return res.status(400).json({ ok: false, error: 'spreadsheetId, range, rows are required' });
  }
  try {
    const data = await appendRows({ spreadsheetId, range, values: rows });
    res.json({ ok: true, data });
  } catch (e) {
    res.status(500).json({ ok: false, error: e?.message || String(e) });
  }
});

app.get('/api/scrape/all', async (req, res) => {
  const { maxPages = 111, batchSize = 10 } = req.query;
  try {
    console.log(`üöÄ Starting full scrape: ${maxPages} pages, batch ${batchSize}`);
    const data = await scrapeAllPagesAndDetails(null, Number(maxPages), Number(batchSize));
    res.json({ ok: true, count: data.length, data });
  } catch (e) {
    res.status(500).json({ ok: false, error: e?.message || String(e) });
  }
});

// Test des liens d'une page
app.get('/api/test/links', async (req, res) => {
  const { page = 1 } = req.query;
  try {
    const url = `https://edv.travel/adherer/annuaire-des-adherents/?pagination=${page}`;
    console.log(`üîó Testing links on page ${page}`);
    const links = await fetchListLinks(url);
    res.json({ 
      ok: true, 
      page: Number(page),
      url,
      totalLinks: links.length, 
      links: links,
      sampleLinks: links.slice(0, 10)
    });
  } catch (e) {
    res.status(500).json({ ok: false, error: e?.message || String(e) });
  }
});

// Scraper TOUTES les informations d'une page compl√®te
app.get('/api/scrape/page', async (req, res) => {
  const { page = 1 } = req.query;
  try {
    const url = `https://edv.travel/adherer/annuaire-des-adherents/?pagination=${page}`;
    console.log(`üöÄ Scraping complete page ${page}: ${url}`);
    
    // √âtape 1: R√©cup√©rer tous les liens de la page
    const links = await fetchListLinks(url);
    console.log(`üìñ Found ${links.length} links to scrape on page ${page}`);
    
    if (links.length === 0) {
      return res.json({ ok: false, error: `No links found on page ${page}` });
    }
    
    // √âtape 2: Scraper toutes les fiches de la page
    const results = [];
    let successful = 0;
    let errors = 0;
    
    for (let i = 0; i < links.length; i++) {
      const link = links[i];
      console.log(`üìÑ Scraping ${i + 1}/${links.length}: ${link}`);
      
      try {
        const data = await scrapeDetail(link);
        results.push(data);
        successful++;
        console.log(`‚úÖ Success: ${data.raison_sociale || 'N/A'}`);
      } catch (error) {
        const errorData = { url: link, error: error.message };
        results.push(errorData);
        errors++;
        console.error(`‚ùå Error: ${link} - ${error.message}`);
      }
      
      // D√©lai optimis√© entre chaque requ√™te
      await new Promise(resolve => setTimeout(resolve, 100));
    }
    
    console.log(`üéâ Page ${page} complete! ${successful} success, ${errors} errors`);
    
    res.json({ 
      ok: true, 
      page: Number(page),
      url,
      totalLinks: links.length,
      successful,
      errors,
      successRate: Math.round((successful / links.length) * 100),
      results
    });
    
  } catch (e) {
    res.status(500).json({ ok: false, error: e?.message || String(e) });
  }
});

// Scraper UNE PAGE ULTRA-RAPIDE (en parall√®le)
app.get('/api/scrape/page/fast', async (req, res) => {
  const { page = 1, concurrency = 5 } = req.query;
  const concurrencyNum = Math.min(Math.max(parseInt(concurrency), 1), 8); // Max 8 en parall√®le
  
  try {
    const url = `https://edv.travel/adherer/annuaire-des-adherents/?pagination=${page}`;
    console.log(`‚ö° Ultra-fast scraping page ${page} with ${concurrencyNum} concurrent requests`);
    
    const startTime = Date.now();
    
    // √âtape 1: R√©cup√©rer tous les liens
    const links = await fetchListLinks(url);
    console.log(`üìñ Found ${links.length} links to scrape`);
    
    if (links.length === 0) {
      return res.json({ ok: false, error: `No links found on page ${page}` });
    }
    
    // √âtape 2: Scraper en parall√®le par batches
    const results = [];
    let successful = 0;
    let errors = 0;
    
    for (let i = 0; i < links.length; i += concurrencyNum) {
      const batch = links.slice(i, i + concurrencyNum);
      const batchNumber = Math.ceil((i + 1) / concurrencyNum);
      const totalBatches = Math.ceil(links.length / concurrencyNum);
      
      console.log(`üîÑ Batch ${batchNumber}/${totalBatches} - ${batch.length} parallel requests`);
      
      // Scraper le batch en parall√®le
      const batchPromises = batch.map(async (link, index) => {
        const globalIndex = i + index + 1;
        try {
          const data = await scrapeDetail(link);
          console.log(`‚úÖ ${globalIndex}/${links.length}: ${data.raison_sociale || 'N/A'}`);
          return { success: true, data };
        } catch (error) {
          console.error(`‚ùå ${globalIndex}/${links.length}: ${error.message}`);
          return { success: false, url: link, error: error.message };
        }
      });
      
      // Attendre que tout le batch soit termin√©
      const batchResults = await Promise.all(batchPromises);
      
      // Traiter les r√©sultats
      batchResults.forEach(result => {
        if (result.success) {
          results.push(result.data);
          successful++;
        } else {
          results.push({ url: result.url, error: result.error });
          errors++;
        }
      });
      
      // Petite pause entre les batches
      if (i + concurrencyNum < links.length) {
        await new Promise(resolve => setTimeout(resolve, 200));
      }
    }
    
    const endTime = Date.now();
    const duration = Math.round((endTime - startTime) / 1000);
    
    console.log(`‚ö° Ultra-fast page ${page} complete in ${duration}s! ${successful} success, ${errors} errors`);
    
    res.json({ 
      ok: true, 
      method: 'ultra-fast-parallel',
      page: Number(page),
      url,
      concurrency: concurrencyNum,
      totalLinks: links.length,
      successful,
      errors,
      successRate: Math.round((successful / links.length) * 100),
      duration,
      speed: Math.round(links.length / (duration / 60)) + ' fiches/min',
      results
    });
    
  } catch (e) {
    res.status(500).json({ ok: false, error: e?.message || String(e) });
  }
});

// Route pour scraper une page et exporter vers Google Sheets
app.post('/api/scrape-simple', async (req, res) => {
  const { page } = req.body || {};
  const pageNum = page || 1;
  
  try {
    console.log(`üöÄ Starting simple scrape for page ${pageNum}...`);
    
    // 1. Scraper la page
    const url = `https://edv.travel/adherer/annuaire-des-adherents?page=${pageNum}`;
    const startTime = Date.now();
    
    const data = await scrapeListPageAndDetails(url, null);
    
    if (!data || data.length === 0) {
      return res.json({ 
        ok: true, 
        message: `Aucune entreprise trouv√©e sur la page ${pageNum}`, 
        count: 0 
      });
    }
    
    // 2. Formater les donn√©es pour Google Sheets (ordre des colonnes exact)
    const rows = data.map(company => [
      company.immatriculation || '',
      company.raison_sociale || '',
      company.enseigne || '',
      company.adresse || '',
      company.code_postal || '',
      company.ville || '',
      company.telephone || '',
      company.fax || '',
      company.email || '',
      company.site_web || ''
    ]);
    
    // 3. Exporter vers Google Sheets
    const spreadsheetId = process.env.GOOGLE_SHEETS_ID;
    if (!spreadsheetId) {
      return res.status(500).json({ 
        ok: false, 
        error: 'GOOGLE_SHEETS_ID not configured in .env' 
      });
    }
    
    const exportResult = await appendRows({
      spreadsheetId,
      range: 'Sheet1!A:J',
      values: rows
    });
    
    const endTime = Date.now();
    const duration = Math.round((endTime - startTime) / 1000);
    
    console.log(`‚úÖ Simple scrape complete! ${data.length} companies exported to Google Sheets`);
    
    res.json({
      ok: true,
      method: 'scrape-simple',
      page: pageNum,
      url,
      count: data.length,
      duration,
      googleSheetsResult: exportResult,
      message: `${data.length} entreprises de la page ${pageNum} export√©es vers Google Sheets`
    });
    
  } catch (e) {
    console.error('‚ùå Error in scrape-simple:', e);
    res.status(500).json({ ok: false, error: e?.message || String(e) });
  }
});

// Route pour scraper plusieurs pages et exporter vers Google Sheets
app.post('/api/scrape-multiple', async (req, res) => {
  const { startPage, endPage } = req.body || {};
  const start = Number(startPage) || 1;
  const end = Number(endPage) || 5;
  
  if (start > end || start < 1 || end > 111) {
    return res.status(400).json({ 
      ok: false, 
      error: 'Invalid page range. Start must be <= end, and pages must be between 1-111' 
    });
  }
  
  try {
    console.log(`üöÄ Starting multiple pages scrape (${start} to ${end})...`);
    
    const startTime = Date.now();
    let allCompanies = [];
    let totalPages = 0;
    let errors = 0;
    
    // Scraper chaque page
    for (let page = start; page <= end; page++) {
      try {
        console.log(`üìÑ Scraping page ${page}/${end}...`);
        const url = `https://edv.travel/adherer/annuaire-des-adherents?page=${page}`;
        const data = await scrapeListPageAndDetails(url, null);
        
        if
