import * as cheerio from 'cheerio';
import { URL } from 'node:url';

// URL de base du site √† scraper
const BASE = 'https://edv.travel';

/**
 * Fonction pour faire une requ√™te GET avec fetch natif de Node.js
 * @param {string} url - URL √† r√©cup√©rer
 * @returns {Promise<string>} - Le contenu HTML de la page
 */
async function httpGet(url) {
  try {
    const response = await fetch(url, {
      headers: {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.8',
        'Connection': 'keep-alive'
      }
    });

    if (!response.ok) {
      throw new Error(`HTTP ${response.status}: ${response.statusText}`);
    }

    return await response.text();
  } catch (error) {
    throw new Error(`Failed to fetch ${url}: ${error.message}`);
  }
}

/**
 * Convertit une URL relative en URL absolue
 * @param {string} href - URL relative ou absolue
 * @returns {string|null} - URL absolue ou null si invalide
 */
function toAbs(href) {
  if (!href) return null;
  try {
    const u = new URL(href, BASE);
    return u.toString();
  } catch {
    return null; // URL invalide
  }
}

/**
 * R√©cup√®re tous les liens vers les fiches adh√©rents depuis la page de liste
 * @param {string} listUrl - URL de la page de liste (optionnel)
 * @returns {Promise<string[]>} - Array des URLs des fiches
 */
export async function fetchListLinks(listUrl) {
  // URL par d√©faut si non fournie
  const url = listUrl || `${BASE}/adherer/annuaire-des-adherents/?pagination=1`;
  
  // R√©cup√©ration du HTML de la page avec notre fonction native
  const html = await httpGet(url);
  const $ = cheerio.load(html);
  
  // Set pour √©viter les doublons de liens
  const links = new Set();
  
  // M√âTHODE 1: S√©lecteur sp√©cifique pour les liens de fiches
  // Tous les liens vers fiche-adherent avec un ID
  $('a[href*="fiche-adherent/?id="]').each((_, el) => {
    const href = $(el).attr('href');
    const abs = toAbs(href);
    if (abs && abs.includes('id=')) {
      links.add(abs);
    }
  });
  
  console.log(`üìã Method 1 - Specific selector found: ${links.size} links`);
  
  // M√âTHODE 2: Si pas assez, chercher TOUS les liens qui contiennent 'id=' et sont des fiches
  if (links.size < 20) {
    console.log(`‚ö†Ô∏è  Only ${links.size} links found with specific selector, trying broader search...`);
    
    $('a').each((_, el) => {
      const href = $(el).attr('href');
      if (href && href.includes('id=') && href.includes('fiche')) {
        const abs = toAbs(href);
        if (abs) {
          links.add(abs);
        }
      }
    });
    
    console.log(`üìã Method 2 - Broader search found: ${links.size} total links`);
  }
  
  // M√âTHODE 3: Si encore pas assez, analyser le contenu des liens
  if (links.size < 20) {
    console.log(`‚ö†Ô∏è  Still only ${links.size} links, trying content-based search...`);
    
    $('a').each((_, el) => {
      const href = $(el).attr('href');
      const text = $(el).text().trim();
      
      // Liens qui ressemblent √† des entreprises (nom + adresse)
      if (href && text && text.length > 20 && text.includes(' - ')) {
        const abs = toAbs(href);
        if (abs && abs.includes('id=')) {
          links.add(abs);
        }
      }
    });
    
    console.log(`ÔøΩ Method 3 - Content-based search found: ${links.size} total links`);
  }
  
  console.log(`üìñ Fetching links from: ${url}`);
  console.log(`üîó Found ${links.size} unique links on this page`);
  
  // Debug: afficher les 5 premiers liens trouv√©s
  const linksArray = Array.from(links);
  console.log(`üìã Sample links found:`, linksArray.slice(0, 5));
  
  return Array.from(links); // Conversion Set -> Array
}

/**
 * Scrape les d√©tails d'une fiche adh√©rent sp√©cifique
 * @param {string} detailUrl - URL de la fiche adh√©rent
 * @returns {Promise<Object>} - Objet contenant toutes les donn√©es extraites
 */
export async function scrapeDetail(detailUrl) {
  // R√©cup√©ration du HTML de la fiche avec notre fonction native
  const html = await httpGet(detailUrl);
  const $ = cheerio.load(html);

  // DEBUG: Log HTML structure for debugging
  console.log(`üåê Scraping: ${detailUrl}`);
  console.log(`üìÑ HTML length: ${html.length} chars`);
  
  // Save HTML to file for inspection (optional debug)
  // import fs from 'node:fs'; fs.writeFileSync('debug.html', html);
  
  console.log(`üè∑Ô∏è  Found ${$('td, th').length} table cells`);
  console.log(`üìã Found ${$('li').length} list items`);
  console.log(`üîó Found ${$('a').length} links`);

  /**
   * Fonction helper pour extraire une valeur par son label
   * @param {string} label - Label √† rechercher (ex: "immatriculation")
   * @returns {string|null} - Valeur trouv√©e ou null
   */
  const getByLabel = (label) => {
    let val = null;
    
    console.log(`\nüîç Searching for: ${label}`);
    
    // M√©thode 1: Chercher dans toutes les balises qui peuvent contenir des labels
    $('*').each((_, el) => {
      const text = $(el).text().trim();
      const lowerText = text.toLowerCase();
      
      if (lowerText.includes(label.toLowerCase()) && text.length < 200) {
        console.log(`  Found potential match: "${text}"`);
        
        // Si c'est un √©l√©ment qui contient EXACTEMENT le label recherch√© (sans autre texte)
        if (lowerText === label.toLowerCase() || lowerText === label.toLowerCase() + ':') {
          console.log(`  üéØ Exact label match found!`);
          
          // Essayer de r√©cup√©rer l'√©l√©ment suivant
          const next = $(el).next();
          if (next.length && next.text().trim()) {
            const nextText = next.text().trim().replace(/\s+/g, ' ');
            console.log(`  ‚úÖ Next element value: "${nextText}"`);
            val = nextText;
            return false;
          }
          
          // Essayer le parent puis le suivant
          const parentNext = $(el).parent().next();
          if (parentNext.length && parentNext.text().trim()) {
            const parentNextText = parentNext.text().trim().replace(/\s+/g, ' ');
            console.log(`  ‚úÖ Parent next value: "${parentNextText}"`);
            val = parentNextText;
            return false;
          }
        }
        
        // Sinon, si le texte contient le label + ":" + valeur dans le m√™me √©l√©ment
        else if (text.includes(':')) {
          const parts = text.split(':');
          if (parts.length >= 2) {
            const labelPart = parts[0].trim().toLowerCase();
            if (labelPart === label.toLowerCase()) {
              const value = parts.slice(1).join(':').trim().replace(/\s+/g, ' ');
              if (value && value.length > 0) {
                console.log(`  ‚úÖ Inline value: "${value}"`);
                val = value;
                return false; // break
              }
            }
          }
        }
      }
    });
    
    console.log(`  Result for ${label}: ${val || 'NULL'}`);
    return val;
  };

  // Extraction de l'email : d'abord par s√©lecteur mailto, sinon par label
  const email = $('a[href^="mailto:"]').first().text().trim() || getByLabel('e-mail');
  
  // Extraction du site web : liens http(s) non-mailto
  const site = $('a[href^="http" i]').filter((_, a) => !$(a).attr('href')?.startsWith('mailto:')).first().attr('href') || getByLabel('site web');

  // Extraction de l'ID depuis l'URL (param√®tre ?id=xxx)
  let id = null;
  try {
    id = new URL(detailUrl).searchParams.get('id');
  } catch { 
    // Ignore les erreurs d'URL
  }

  // Retourne un objet structur√© avec toutes les donn√©es
  return {
    id,                                                    // ID de la fiche
    url: detailUrl,                                       // URL source
    immatriculation: getByLabel('immatriculation') || null,   // Num√©ro d'immatriculation
    raison_sociale: getByLabel('raison sociale') || null,    // Nom de l'entreprise
    enseigne: getByLabel('enseigne') || null,               // Nom commercial
    adresse: getByLabel('adresse') || null,                 // Adresse compl√®te
    code_postal: getByLabel('code postal') || null,         // Code postal
    ville: getByLabel('ville') || null,                     // Ville
    telephone: getByLabel('t√©l√©phone') || getByLabel('telephone') || null, // T√©l√©phone (avec/sans accent)
    email: email || null,                                   // Email
    site_web: site || null,                                 // Site web
  };

  console.log(`Scraped detail: ${detailUrl}`);
}

/**
 * R√©cup√®re tous les liens de toutes les pages (avec pagination)
 * @param {string} baseUrl - URL de base de la liste
 * @param {number} maxPages - Nombre maximum de pages √† scraper (d√©faut: 111)
 * @returns {Promise<string[]>} - Array de tous les URLs des fiches
 */
export async function fetchAllListLinks(baseUrl, maxPages = 111) {
  const baseListUrl = baseUrl || `${BASE}/adherer/annuaire-des-adherents`;
  const allLinks = new Set();
  
  console.log(`üöÄ Starting to scrape ${maxPages} pages...`);
  
  for (let page = 1; page <= maxPages; page++) {
    try {
      const pageUrl = `${baseListUrl}/?pagination=${page}`;
      console.log(`üìñ Scraping page ${page}/${maxPages}: ${pageUrl}`);
      
      // R√©cup√©rer les liens de cette page
      const pageLinks = await fetchListLinks(pageUrl);
      
      if (pageLinks.length === 0) {
        console.log(`‚ö†Ô∏è  Page ${page} has no links, stopping pagination`);
        break;
      }
      
      // Ajouter tous les liens trouv√©s
      pageLinks.forEach(link => allLinks.add(link));
      console.log(`‚úÖ Page ${page}: found ${pageLinks.length} links (total: ${allLinks.size})`);
      
      // D√©lai pour √©viter de surcharger le serveur
      if (page < maxPages) {
        await new Promise(resolve => setTimeout(resolve, 1000)); // 1 seconde entre chaque page
      }
      
    } catch (error) {
      console.error(`‚ùå Error scraping page ${page}:`, error.message);
      // Continuer avec la page suivante m√™me en cas d'erreur
    }
  }
  
  console.log(`üéâ Pagination complete! Found ${allLinks.size} unique links across ${maxPages} pages`);
  return Array.from(allLinks);
}

/**
 * Scrape TOUTES les pages ET toutes les fiches d√©tail
 * @param {string} baseUrl - URL de base de la liste 
 * @param {number} maxPages - Nombre maximum de pages (d√©faut: 111)
 * @param {number} batchSize - Nombre de fiches √† traiter par batch (d√©faut: 10)
 * @returns {Promise<Object[]>} - Array d'objets contenant les donn√©es ou erreurs
 */
export async function scrapeAllPagesAndDetails(baseUrl, maxPages = 111, batchSize = 10) {
  console.log(`üåç Starting full scrape: ${maxPages} pages, batch size: ${batchSize}`);
  
  // √âtape 1: R√©cup√©ration de tous les liens de toutes les pages
  const allLinks = await fetchAllListLinks(baseUrl, maxPages);
  console.log(`üìä Total links to scrape: ${allLinks.length}`);
  
  const results = [];
  
  // √âtape 2: Scraping par batch pour √©viter de surcharger
  for (let i = 0; i < allLinks.length; i += batchSize) {
    const batch = allLinks.slice(i, i + batchSize);
    const batchNumber = Math.floor(i / batchSize) + 1;
    const totalBatches = Math.ceil(allLinks.length / batchSize);
    
    console.log(`üîÑ Processing batch ${batchNumber}/${totalBatches} (${batch.length} items)`);
    
    // Scraper chaque fiche du batch
    for (const url of batch) {
      try {
        const data = await scrapeDetail(url);
        results.push(data);
        console.log(`‚úÖ Scraped ${results.length}/${allLinks.length}: ${data.raison_sociale || 'N/A'}`);
      } catch (error) {
        console.error(`‚ùå Error scraping ${url}:`, error.message);
        results.push({ url, error: error.message });
      }
      
      // Petit d√©lai entre chaque requ√™te
      await new Promise(resolve => setTimeout(resolve, 500)); // 0.5 seconde
    }
    
    console.log(`‚úÖ Batch ${batchNumber} complete. Total scraped: ${results.length}`);
  }
  
  console.log(`üéä Full scrape complete! Scraped ${results.length} records`);
  return results;
}

/**
 * Scrape une page de liste ET toutes les fiches d√©tail associ√©es
 * @param {string} listUrl - URL de la page de liste
 * @param {number} limit - Nombre maximum de fiches √† traiter (d√©faut: 50)
 * @returns {Promise<Object[]>} - Array d'objets contenant les donn√©es ou erreurs
 */
export async function scrapeListPageAndDetails(listUrl, limit = 50) {
  // √âtape 1: R√©cup√©ration de tous les liens
  const links = await fetchListLinks(listUrl);
  
  // Limitation du nombre de fiches √† traiter
  const slice = links.slice(0, limit);
  const results = [];
  
  // √âtape 2: Scraping de chaque fiche individuellement
  for (const u of slice) {
    try {
      const d = await scrapeDetail(u); // Scrape la fiche
      results.push(d);
    } catch (e) {
      // En cas d'erreur, on ajoute l'URL et l'erreur
      results.push({ url: u, error: e?.message || String(e) });
    }
  }

  console.log(`Scraped ${results.length} details from list page.`);
  
  return results;
}